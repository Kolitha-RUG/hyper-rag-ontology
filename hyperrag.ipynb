{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "046ff76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollama in .\\venv\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: rdflib in .\\venv\\lib\\site-packages (7.1.4)\n",
      "Requirement already satisfied: networkx in .\\venv\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: faiss-cpu in .\\venv\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: pandas in .\\venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in .\\venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: rapidfuzz in .\\venv\\lib\\site-packages (3.13.0)\n",
      "Requirement already satisfied: pyshacl in .\\venv\\lib\\site-packages (0.30.1)\n",
      "Collecting json5\n",
      "  Downloading json5-0.12.1-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in .\\venv\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in .\\venv\\lib\\site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in .\\venv\\lib\\site-packages (from rdflib) (3.2.3)\n",
      "Requirement already satisfied: packaging in .\\venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: owlrl<8,>=7.1.2 in .\\venv\\lib\\site-packages (from pyshacl) (7.1.4)\n",
      "Requirement already satisfied: prettytable>=3.7.0 in .\\venv\\lib\\site-packages (from pyshacl) (3.16.0)\n",
      "Requirement already satisfied: html5rdf<2,>=1.2 in .\\venv\\lib\\site-packages (from rdflib[html]!=7.1.2,<8.0,>=7.1.1->pyshacl) (1.2.1)\n",
      "Requirement already satisfied: anyio in .\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in .\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in .\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in .\\venv\\lib\\site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in .\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: wcwidth in .\\venv\\lib\\site-packages (from prettytable>=3.7.0->pyshacl) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in .\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in .\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in .\\venv\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in .\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in .\\venv\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Downloading json5-0.12.1-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: json5\n",
      "Successfully installed json5-0.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install ollama rdflib networkx faiss-cpu pandas numpy rapidfuzz pyshacl json5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "013bd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "\n",
    "import os, re, json, time, uuid, glob\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ollama\n",
    "import rdflib\n",
    "from rdflib import Graph\n",
    "from rdflib.namespace import RDF, RDFS, OWL, SKOS, XSD\n",
    "import networkx as nx\n",
    "import faiss\n",
    "from rapidfuzz import fuzz\n",
    "from string import Template\n",
    "import json5\n",
    "\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    na = np.linalg.norm(a) + 1e-9\n",
    "    nb = np.linalg.norm(b) + 1e-9\n",
    "    return float((a @ b) / (na * nb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "140050fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM: mistral:latest | EMBED: nomic-embed-text\n",
      "Data: D:\\hyper-rag-ontology\\data | Output: D:\\hyper-rag-ontology\\outputs\n"
     ]
    }
   ],
   "source": [
    "#Config\n",
    "DATA_DIR = Path(\"data\")\n",
    "ONTOLOGY_TTL = DATA_DIR / \"hcdt.ttl\"\n",
    "CORPUS_DIR = DATA_DIR / \"corpus\"\n",
    "\n",
    "LLM_MODEL   = os.environ.get(\"OLLAMA_LLM\", \"mistral:latest\")\n",
    "EMBED_MODEL = os.environ.get(\"OLLAMA_EMBED\", \"nomic-embed-text\")\n",
    "\n",
    "OUT_DIR = Path(\"outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "K_ENTITY = 60\n",
    "K_HYPEREDGE = 60\n",
    "ENTITY_ALIGN_MIN = 0.80\n",
    "NEW_CONCEPT_FREQ_MIN = 2\n",
    "\n",
    "DATA_DIR.mkdir(exist_ok=True); CORPUS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"LLM:\", LLM_MODEL, \"| EMBED:\", EMBED_MODEL)\n",
    "print(\"Data:\", DATA_DIR.resolve(), \"| Output:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa38e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Check Ollama\n",
    "# def check_ollama():\n",
    "#     try:\n",
    "#         ms = ollama.list()\n",
    "#         names = [m.get(\"name\") for m in ms.get(\"models\", [])]\n",
    "#         print(\"Ollama models:\", names)\n",
    "#         if LLM_MODEL not in names:\n",
    "#             print(f\"Note: pull chat model ->  ollama pull {LLM_MODEL}\")\n",
    "#         if EMBED_MODEL not in names:\n",
    "#             print(f\"Note: pull embed model -> ollama pull {EMBED_MODEL}\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(\"Could not reach Ollama. Run: `ollama serve`\")\n",
    "#         raise\n",
    "# _ = check_ollama()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff19cfca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classes=51, obj_props=15, dt_props=4\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Load upper ontology (TTL → catalogs)\n",
    "g = Graph(); g.parse(str(ONTOLOGY_TTL))\n",
    "RDFS = rdflib.namespace.RDFS; OWL = rdflib.namespace.OWL; SKOS = rdflib.namespace.SKOS\n",
    "\n",
    "def lit(s): return str(s) if s is not None else \"\"\n",
    "\n",
    "def label(graph, node):\n",
    "    for l in graph.objects(node, RDFS.label): return lit(l)\n",
    "    return None\n",
    "\n",
    "def alts(graph, node):\n",
    "    return [lit(a) for a in graph.objects(node, SKOS.altLabel)]\n",
    "\n",
    "classes = [{\"iri\": str(c), \"label\": label(g,c) or str(c), \"alt\": alts(g,c)}\n",
    "           for c in g.subjects(rdflib.RDF.type, OWL.Class)]\n",
    "\n",
    "obj_props = [{\"iri\": str(p), \"label\": label(g,p) or str(p), \"alt\": alts(g,p)} \n",
    "             for p in g.subjects(rdflib.RDF.type, OWL.ObjectProperty)]\n",
    "\n",
    "dt_props  = [{\"iri\": str(p), \"label\": label(g,p) or str(p), \"alt\": alts(g,p)} \n",
    "             for p in g.subjects(rdflib.RDF.type, OWL.DatatypeProperty)]\n",
    "\n",
    "print(f\"Loaded classes={len(classes)}, obj_props={len(obj_props)}, dt_props={len(dt_props)}\")\n",
    "ONTO = {\"classes\": classes, \"obj_props\": obj_props, \"dt_props\": dt_props}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96896062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: 3\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Load Markdown corpus\n",
    "\n",
    "\n",
    "DOCS = [{\"path\": str(p), \"text\": p.read_text(encoding=\"utf-8\", errors=\"ignore\")}\n",
    "        for p in sorted(CORPUS_DIR.glob(\"*.md\"))]\n",
    "print(\"Docs:\", len(DOCS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ace8c5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chunk_markdown(\n",
    "    text: str,\n",
    "    max_chars: int = 6000,      # ~1–1.3k tokens (rough rule)\n",
    "    min_chars: int = 2000,      # avoid tiny chunks\n",
    "    overlap_chars: int = 500    # ~10% overlap\n",
    "):\n",
    "    # Split into blocks by Markdown structure: headings, lists, code fences, paragraphs\n",
    "    fence = False\n",
    "    blocks = []\n",
    "    buf = []\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        if line.strip().startswith(\"```\"):\n",
    "            fence = not fence\n",
    "            buf.append(line)\n",
    "            continue\n",
    "\n",
    "        if fence:\n",
    "            buf.append(line)\n",
    "            continue\n",
    "\n",
    "        if re.match(r\"^#{1,6}\\s\", line) or re.match(r\"^\\s*[-*+]\\s\", line) or line.strip() == \"\":\n",
    "            if buf:\n",
    "                blocks.append(\"\\n\".join(buf).strip())\n",
    "                buf = []\n",
    "            if line.strip():  # keep non-empty separators like headings as standalone blocks\n",
    "                blocks.append(line)\n",
    "        else:\n",
    "            buf.append(line)\n",
    "    if buf:\n",
    "        blocks.append(\"\\n\".join(buf).strip())\n",
    "\n",
    "    # Pack blocks into sized chunks with overlap\n",
    "    chunks = []\n",
    "    cur, n = [], 0\n",
    "    for b in blocks:\n",
    "        b = b.strip()\n",
    "        if not b:\n",
    "            continue\n",
    "        if n + len(b) + 1 > max_chars and n >= min_chars:\n",
    "            chunk = \"\\n\".join(cur).strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            # Overlap: carry tail\n",
    "            tail = chunk[-overlap_chars:]\n",
    "            cur = [tail, b]\n",
    "            n = len(tail) + len(b) + 1\n",
    "        else:\n",
    "            cur.append(b)\n",
    "            n += len(b) + 1\n",
    "    if cur:\n",
    "        chunks.append(\"\\n\".join(cur).strip())\n",
    "\n",
    "    # Final cleanup: drop accidental empties\n",
    "    return [c for c in chunks if c and len(c.strip()) > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1282b0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperedges per doc: [1, 7, 147]\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — N-ary extraction (hyperedges + entities) via Ollama\n",
    "EXTRACTION_PROMPT_TMPL = Template(\"\"\"\n",
    "You extract n-ary relational facts (hyperedges) and entities.\n",
    "\n",
    "Return ONE JSON object ONLY. No prose, no extra blocks, no code fences.\n",
    "\n",
    "{\n",
    "  \"hyperedges\": [\n",
    "    {\n",
    "      \"text\": \"...\",\n",
    "      \"score\": 0.0,\n",
    "      \"entities\": [\n",
    "        { \"name\": \"...\", \"type\": \"...\", \"description\": \"...\", \"key_score\": 0.0 }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "- Split the input into coherent knowledge fragments (as hyperedges).\n",
    "- Include all entities per hyperedge.\n",
    "- Keep scores as floats.\n",
    "- Same language as the input.\n",
    "\n",
    "INPUT:\n",
    "---\n",
    "$chunk\n",
    "---\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "def _strip_code_fences(s: str) -> str:\n",
    "    s = s.strip()\n",
    "    if s.startswith(\"```\"):\n",
    "        # remove leading and trailing fenced blocks\n",
    "        s = s.strip(\"`\")\n",
    "        # often starts with ```json\n",
    "        if s.lower().startswith(\"json\"):\n",
    "            s = s[4:].lstrip()\n",
    "    return s\n",
    "\n",
    "def _clean_common_json_issues(s: str) -> str:\n",
    "    # normalize smart quotes\n",
    "    s = s.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\").replace(\"’\", \"'\")\n",
    "    # remove trailing commas before } or ]\n",
    "    s = re.sub(r',\\s*([}\\]])', r'\\1', s)\n",
    "    # remove BOM if present\n",
    "    s = s.lstrip(\"\\ufeff\")\n",
    "    return s\n",
    "\n",
    "def _extract_json_objects(s: str) -> list[str]:\n",
    "    \"\"\"Extract top-level {...} blocks from a string that may contain multiple JSON objects.\"\"\"\n",
    "    objs, in_str, esc, depth, start = [], False, False, 0, None\n",
    "    for i, ch in enumerate(s):\n",
    "        if in_str:\n",
    "            if esc:\n",
    "                esc = False\n",
    "            elif ch == '\\\\':\n",
    "                esc = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "        else:\n",
    "            if ch == '\"':\n",
    "                in_str = True\n",
    "            elif ch == '{':\n",
    "                if depth == 0:\n",
    "                    start = i\n",
    "                depth += 1\n",
    "            elif ch == '}':\n",
    "                depth -= 1\n",
    "                if depth == 0 and start is not None:\n",
    "                    objs.append(s[start:i+1])\n",
    "                    start = None\n",
    "    return objs\n",
    "\n",
    "def _try_load_json(s: str):\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        if json5 is not None:\n",
    "            try:\n",
    "                return json5.loads(s)\n",
    "            except Exception:\n",
    "                pass\n",
    "    # final attempt: clean again then try std json\n",
    "    s2 = _clean_common_json_issues(s)\n",
    "    try:\n",
    "        return json.loads(s2)\n",
    "    except Exception:\n",
    "        if json5 is not None:\n",
    "            try:\n",
    "                return json5.loads(s2)\n",
    "            except Exception:\n",
    "                pass\n",
    "    raise\n",
    "\n",
    "def ollama_json(prompt: str, model=LLM_MODEL, tries=2) -> dict:\n",
    "    last_err = None\n",
    "    for _ in range(tries + 1):\n",
    "        # Prefer strict JSON mode; some models may ignore it\n",
    "        try:\n",
    "            resp = ollama.generate(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                options={\"temperature\": 0.2, \"format\": \"json\"}\n",
    "            )\n",
    "        except Exception:\n",
    "            resp = ollama.generate(model=model, prompt=prompt, options={\"temperature\": 0.2})\n",
    "\n",
    "        raw = resp.get(\"response\", \"\").strip()\n",
    "        txt = _strip_code_fences(raw)\n",
    "        txt = _clean_common_json_issues(txt)\n",
    "\n",
    "        # 1) If it’s a single JSON object, parse directly\n",
    "        try:\n",
    "            obj = _try_load_json(txt)\n",
    "            if isinstance(obj, dict) and \"hyperedges\" in obj:\n",
    "                return obj\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "\n",
    "        # 2) Otherwise, merge multiple top-level JSON objects\n",
    "        merged = {\"hyperedges\": []}\n",
    "        ok = False\n",
    "        for chunk in _extract_json_objects(txt):\n",
    "            try:\n",
    "                one = _try_load_json(_clean_common_json_issues(chunk))\n",
    "                if isinstance(one, dict) and \"hyperedges\" in one and isinstance(one[\"hyperedges\"], list):\n",
    "                    merged[\"hyperedges\"].extend(one[\"hyperedges\"])\n",
    "                    ok = True\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "        if ok:\n",
    "            return merged\n",
    "\n",
    "        time.sleep(0.3)\n",
    "\n",
    "    # Debug help: write the raw text so you can inspect the failing case\n",
    "    dbg_path = OUT_DIR / \"failed_ollama_response.txt\"\n",
    "    dbg_path.write_text(raw, encoding=\"utf-8\")\n",
    "    raise RuntimeError(f\"Failed to parse JSON from model output. Last error: {last_err}. \"\n",
    "                       f\"Saved raw response to: {dbg_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def extract_hypergraph(text: str, max_chars=2500):\n",
    "    chunks = chunk_markdown(text, max_chars=max_chars, min_chars=2000, overlap_chars=500)\n",
    "    # chunks = chunk_sentences(text, max_chars=4500, overlap_sentences=2)\n",
    "    # chunks = chunk_semantic(text, embed_model=EMBED_MODEL, target_chars=5000, min_chars=1500)\n",
    "\n",
    "    out = []\n",
    "    for ch in chunks:\n",
    "        prompt = EXTRACTION_PROMPT_TMPL.substitute(chunk=ch)\n",
    "        obj = ollama_json(prompt)\n",
    "        for he in obj.get(\"hyperedges\", []):\n",
    "            he[\"id\"] = str(uuid.uuid4())\n",
    "            for ent in he.get(\"entities\", []):\n",
    "                ent[\"id\"] = str(uuid.uuid4())\n",
    "            out.append(he)\n",
    "    return out\n",
    "\n",
    "# cache\n",
    "CACHE = OUT_DIR / \"extraction.json\"\n",
    "if CACHE.exists():\n",
    "    data = json.loads(CACHE.read_text())\n",
    "else:\n",
    "    data = {\"docs\":[]}\n",
    "    for d in DOCS:\n",
    "        hyperedges = extract_hypergraph(d[\"text\"])\n",
    "        data[\"docs\"].append({\"path\": d[\"path\"], \"hyperedges\": hyperedges})\n",
    "    CACHE.write_text(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"Hyperedges per doc:\", [len(d[\"hyperedges\"]) for d in data[\"docs\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f148f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities=478 Hyperedges=155 Edges=478\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Build bipartite hypergraph + embeddings + FAISS\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "ENTITY_NODES, HYPER_NODES = {}, {}\n",
    "\n",
    "def add_hyperedge(doc_path, he):\n",
    "    hid = he[\"id\"]\n",
    "    HYPER_NODES[hid] = {\"id\": hid, \"text\": he.get(\"text\",\"\"), \"score\": float(he.get(\"score\",0)), \"doc\": doc_path}\n",
    "    G.add_node(hid, kind=\"hyperedge\")\n",
    "    for ent in he.get(\"entities\", []):\n",
    "        eid = ent[\"id\"]\n",
    "        if eid not in ENTITY_NODES:\n",
    "            ENTITY_NODES[eid] = {\"id\": eid, \"name\": ent.get(\"name\",\"\"), \"type\": ent.get(\"type\",\"Entity\"),\n",
    "                                 \"description\": ent.get(\"description\",\"\"), \"key_score\": float(ent.get(\"key_score\",0))}\n",
    "            G.add_node(eid, kind=\"entity\")\n",
    "        G.add_edge(hid, eid, kind=\"MENTIONS\")\n",
    "\n",
    "for doc in data[\"docs\"]:\n",
    "    for he in doc[\"hyperedges\"]:\n",
    "        add_hyperedge(doc[\"path\"], he)\n",
    "\n",
    "print(f\"Entities={len(ENTITY_NODES)} Hyperedges={len(HYPER_NODES)} Edges={G.number_of_edges()}\")\n",
    "\n",
    "def embed_texts(texts: List[str], model=EMBED_MODEL):\n",
    "    vecs=[]\n",
    "    for t in texts:\n",
    "        r = ollama.embeddings(model=model, prompt=t)\n",
    "        vecs.append(np.array(r[\"embedding\"], dtype=np.float32))\n",
    "    return np.vstack(vecs) if vecs else np.zeros((0,768), dtype=np.float32)\n",
    "\n",
    "entity_ids = list(ENTITY_NODES.keys())\n",
    "hyper_ids  = list(HYPER_NODES.keys())\n",
    "\n",
    "E_TEXTS = [ENTITY_NODES[e][\"name\"] + \" :: \" + ENTITY_NODES[e][\"description\"] for e in entity_ids]\n",
    "H_TEXTS = [HYPER_NODES[h][\"text\"] for h in hyper_ids]\n",
    "\n",
    "E_MAT = embed_texts(E_TEXTS)\n",
    "H_MAT = embed_texts(H_TEXTS)\n",
    "\n",
    "def build_index(mat):\n",
    "    if mat.size == 0: return None\n",
    "    d = mat.shape[1]\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "    matn = mat / (np.linalg.norm(mat,axis=1,keepdims=True)+1e-9)\n",
    "    index.add(matn.astype(np.float32))\n",
    "    return index\n",
    "\n",
    "E_INDEX = build_index(E_MAT)\n",
    "H_INDEX = build_index(H_MAT)\n",
    "\n",
    "def embed_query(q:str):\n",
    "    r = ollama.embeddings(model=EMBED_MODEL, prompt=q)\n",
    "    return np.array(r[\"embedding\"], dtype=np.float32)\n",
    "\n",
    "def faiss_search(index, mat, qvec, k=10):\n",
    "    qn = qvec / (np.linalg.norm(qvec)+1e-9)\n",
    "    D, I = index.search(qn[np.newaxis,:].astype(np.float32), k)\n",
    "    return D[0], I[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e4eb16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Dual retrieval + expansion\n",
    "def retrieve_entities(query:str, k=K_ENTITY):\n",
    "    if E_INDEX is None: return []\n",
    "    qv = embed_query(query)\n",
    "    D,I = faiss_search(E_INDEX, E_MAT, qv, k=min(k, len(entity_ids)))\n",
    "    out=[]\n",
    "    for d,i in zip(D,I):\n",
    "        if i<0: continue\n",
    "        eid = entity_ids[int(i)]\n",
    "        out.append({\"id\": eid, \"score\": float(d), **ENTITY_NODES[eid]})\n",
    "    return out\n",
    "\n",
    "def retrieve_hyperedges(query:str, k=K_HYPEREDGE):\n",
    "    if H_INDEX is None: return []\n",
    "    qv = embed_query(query)\n",
    "    D,I = faiss_search(H_INDEX, H_MAT, qv, k=min(k, len(hyper_ids)))\n",
    "    out=[]\n",
    "    for d,i in zip(D,I):\n",
    "        if i<0: continue\n",
    "        hid = hyper_ids[int(i)]\n",
    "        out.append({\"id\": hid, \"score\": float(d), **HYPER_NODES[hid]})\n",
    "    return out\n",
    "\n",
    "def expand_from_entities(eids: List[str]):\n",
    "    facts=[]\n",
    "    for eid in eids:\n",
    "        for nbr in G.neighbors(eid):\n",
    "            if G.nodes[nbr].get(\"kind\")==\"hyperedge\":\n",
    "                ents=[n for n in G.neighbors(nbr) if G.nodes[n].get(\"kind\")==\"entity\"]\n",
    "                facts.append({\"hyperedge\": HYPER_NODES[nbr], \"entities\":[ENTITY_NODES[x] for x in ents]})\n",
    "    return facts\n",
    "\n",
    "def expand_from_hyperedges(hids: List[str]):\n",
    "    facts=[]\n",
    "    for hid in hids:\n",
    "        ents=[n for n in G.neighbors(hid) if G.nodes[n].get(\"kind\")==\"entity\"]\n",
    "        facts.append({\"hyperedge\": HYPER_NODES[hid], \"entities\":[ENTITY_NODES[x] for x in ents]})\n",
    "    return facts\n",
    "\n",
    "def fused_retrieval(query:str, k_entity=K_ENTITY, k_hyper=K_HYPEREDGE):\n",
    "    ents = retrieve_entities(query, k_entity)\n",
    "    hyps = retrieve_hyperedges(query, k_hyper)\n",
    "    eids = [e[\"id\"] for e in ents]\n",
    "    hids = [h[\"id\"] for h in hyps]\n",
    "    facts = expand_from_entities(eids) + expand_from_hyperedges(hids)\n",
    "    uniq, out = set(), []\n",
    "    for f in facts:\n",
    "        hid = f[\"hyperedge\"][\"id\"]\n",
    "        if hid not in uniq:\n",
    "            uniq.add(hid); out.append(f)\n",
    "    return ents, hyps, out\n",
    "\n",
    "# smoke test\n",
    "# e,h,f = fused_retrieval(\"What are the actors?\", 10, 10)\n",
    "# print(len(e), len(h), len(f))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0ac9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 — Prepare ontology label embeddings\n",
    "def flatten_labels(items):\n",
    "    out=[]\n",
    "    for it in items:\n",
    "        variants = [it.get(\"label\")] + it.get(\"alt\", [])\n",
    "        variants = [v for v in variants if v]\n",
    "        text = \" || \".join(dict.fromkeys(variants)) if variants else it[\"iri\"]\n",
    "        out.append({\"iri\": it[\"iri\"], \"text\": text})\n",
    "    return out\n",
    "\n",
    "CLASS_LABELS = flatten_labels(ONTO[\"classes\"])\n",
    "OBJPROP_LABELS = flatten_labels(ONTO[\"obj_props\"])\n",
    "DTPROP_LABELS  = flatten_labels(ONTO[\"dt_props\"])\n",
    "\n",
    "def embed_list_texts(rows): \n",
    "    return embed_texts([r[\"text\"] for r in rows]) if rows else np.zeros((0,0), dtype=np.float32)\n",
    "\n",
    "CLS_MAT = embed_list_texts(CLASS_LABELS)\n",
    "OP_MAT  = embed_list_texts(OBJPROP_LABELS)\n",
    "DP_MAT  = embed_list_texts(DTPROP_LABELS)\n",
    "\n",
    "def build_idx(mat):\n",
    "    if mat.size==0: return None\n",
    "    idx = faiss.IndexFlatIP(mat.shape[1])\n",
    "    idx.add((mat/(np.linalg.norm(mat,axis=1,keepdims=True)+1e-9)).astype(np.float32))\n",
    "    return idx\n",
    "\n",
    "CLS_IDX = build_idx(CLS_MAT)\n",
    "OP_IDX  = build_idx(OP_MAT)\n",
    "DP_IDX  = build_idx(DP_MAT)\n",
    "\n",
    "def nearest(mat, idx, qvec, k=5):\n",
    "    if idx is None or mat.size==0: return []\n",
    "    qn = qvec / (np.linalg.norm(qvec)+1e-9)\n",
    "    D,I = idx.search(qn[np.newaxis,:].astype(np.float32), k)\n",
    "    return list(zip(D[0].tolist(), I[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7eb2a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Alignment helpers\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "def lexical_sim(a:str, b:str)->float:\n",
    "    if not a or not b: return 0.0\n",
    "    return 0.01 * fuzz.token_set_ratio(a.lower(), b.lower())  # 0..1\n",
    "\n",
    "def align_entity_to_class(ent:Dict[str,Any], topk=5, alpha=0.65):\n",
    "    text = (ent.get(\"name\",\"\") + \" :: \" + ent.get(\"description\",\"\")).strip()\n",
    "    qv = embed_query(text)\n",
    "    nns = nearest(CLS_MAT, CLS_IDX, qv, k=topk)\n",
    "    out=[]\n",
    "    for score, i in nns:\n",
    "        cls = CLASS_LABELS[int(i)]\n",
    "        lex = lexical_sim(text, cls[\"text\"])\n",
    "        comb = float(alpha*score + (1-alpha)*lex)\n",
    "        out.append({\"class_iri\": cls[\"iri\"], \"class_text\": cls[\"text\"], \"embed\": float(score), \"lex\": float(lex), \"score\": comb})\n",
    "    return sorted(out, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "def suggest_property(ent_type_or_label:str, prefer_dt=False, topk=3):\n",
    "    if prefer_dt:\n",
    "        pool, mat, idx = DTPROP_LABELS, DP_MAT, DP_IDX\n",
    "    else:\n",
    "        pool, mat, idx = OBJPROP_LABELS, OP_MAT, OP_IDX\n",
    "    if mat.size==0 or idx is None: return []\n",
    "    qv = embed_query(ent_type_or_label)\n",
    "    nns = nearest(mat, idx, qv, k=topk)\n",
    "    return [{\"prop_iri\": pool[i][\"iri\"], \"prop_text\": pool[i][\"text\"], \"embed\": float(s)} for s,i in nns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "979b2854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":Situation_0a7ae195 a :ClinicalSituation .\n",
      ":59fa5e8a_Heart_rate_data a <http://example.org/onto#Heart_rate_data> ; rdfs:label \"Heart rate data\" .\n",
      ":f4ca9ce2_138_21_BPM a <http://example.org/onto#138_21_BPM> ; rdfs:label \"138.21 BPM\" .\n",
      ":5931d529_77_09_BPM a <http://example.org/onto#77_09_BPM> ; rdfs:label \"77.09 BPM\" .\n",
      ":Situation_0a7ae195 <http://www.kolitha.com/hcdt/hasLatencyValue> :59fa5e8a_Heart \n",
      "---\n",
      " :138_21_BPM a owl:Class ; rdfs:label \"138.21 BPM\" .\n",
      ":77_09_BPM a owl:Class ; rdfs:label \"77.09 BPM\" .\n",
      ":Heart_rate_data a owl:Class ; rdfs:label \"Heart rate data\" .\n"
     ]
    }
   ],
   "source": [
    "def iri_safe(s:str)->str:\n",
    "    return re.sub(r'[^A-Za-z0-9_]+', '_', s).strip('_') or (\"X_\"+uuid.uuid4().hex[:6])\n",
    "\n",
    "BASE = \"http://example.org/onto#\"\n",
    "\n",
    "def propose_axioms_for_fact(fact:Dict[str,Any], cls_threshold=ENTITY_ALIGN_MIN):\n",
    "    he = fact[\"hyperedge\"]; ents = fact[\"entities\"]\n",
    "    situation_id = \"Situation_\" + he[\"id\"].split(\"-\")[0]\n",
    "    abox, tbox, new_classes = [], [], []\n",
    "\n",
    "    abox.append(f\":{situation_id} a :ClinicalSituation .\")\n",
    "\n",
    "    bindings=[]\n",
    "    for e in ents:\n",
    "        label = e[\"name\"] or e[\"type\"]\n",
    "        eid = f\"{e['id'].split('-')[0]}_{iri_safe(label)[:24]}\"\n",
    "        best = align_entity_to_class(e, topk=5)\n",
    "        if best and best[0][\"score\"] >= cls_threshold:\n",
    "            cls_iri = best[0][\"class_iri\"]\n",
    "        else:\n",
    "            new_cls = f\":{iri_safe(label)}\"\n",
    "            tbox.append(f\"{new_cls} a owl:Class ; rdfs:label \\\"{label}\\\" .\")\n",
    "            cls_iri = f\"{BASE}{iri_safe(label)}\"\n",
    "            new_classes.append({\"proposed\": cls_iri, \"label\": label})\n",
    "        abox.append(f\":{eid} a <{cls_iri}> ; rdfs:label \\\"{label}\\\" .\")\n",
    "        bindings.append({\"id\": eid, \"label\": label, \"type\": e.get(\"type\",\"\")})\n",
    "\n",
    "    for b in bindings:\n",
    "        prefer_dt = \"measure\" in (b[\"type\"] or b[\"label\"]).lower()\n",
    "        props = suggest_property(b[\"type\"] or b[\"label\"], prefer_dt=prefer_dt, topk=1)\n",
    "        if props:\n",
    "            piri = props[0][\"prop_iri\"]\n",
    "        else:\n",
    "            piri = f\"{BASE}relatedTo\"\n",
    "            tbox.append(f\"<{piri}> a owl:ObjectProperty ; rdfs:label \\\"related to\\\" .\")\n",
    "        abox.append(f\":{situation_id} <{piri}> :{b['id']} .\")\n",
    "\n",
    "    abox.append(f\":{situation_id} rdfs:comment \\\"{he['text'].replace('\\\"','\\\\\\\"')}\\\" .\")\n",
    "    return {\"abox\":\"\\n\".join(abox), \"tbox\":\"\\n\".join(sorted(set(tbox))), \"situation\": situation_id, \"new_classes\": new_classes}\n",
    "\n",
    "# Try on first fused fact\n",
    "_,_,facts = fused_retrieval(\"hypertension creatinine mild male\", 10, 10)\n",
    "if facts:\n",
    "    demo = propose_axioms_for_fact(facts[0])\n",
    "    print(demo[\"abox\"][:400], \"\\n---\\n\", demo[\"tbox\"][:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5108629c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>freq</th>\n",
       "      <th>best_align</th>\n",
       "      <th>parent_iri</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UAV</td>\n",
       "      <td>11</td>\n",
       "      <td>0.407679</td>\n",
       "      <td>http://www.kolitha.com/hcdt/sosa:Sensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVIDIA Isaac Sim</td>\n",
       "      <td>8</td>\n",
       "      <td>0.457461</td>\n",
       "      <td>http://www.kolitha.com/hcdt/sosa:Sensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UAVs</td>\n",
       "      <td>7</td>\n",
       "      <td>0.416369</td>\n",
       "      <td>http://www.sws.org/sws/Ergonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Middleware</td>\n",
       "      <td>5</td>\n",
       "      <td>0.457772</td>\n",
       "      <td>http://www.kolitha.com/hcdt/DataVisualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UC-AGR-2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.493644</td>\n",
       "      <td>http://www.sws.org/sws/AIAcceptance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DJI Phantom 4 Multispectral</td>\n",
       "      <td>4</td>\n",
       "      <td>0.422528</td>\n",
       "      <td>http://www.kolitha.com/hcdt/DataVisualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UC-AGR-3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.434973</td>\n",
       "      <td>http://www.kolitha.com/hcdt/NonHumanActor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UC-AGR-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.450762</td>\n",
       "      <td>http://www.sws.org/sws/AIAcceptance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vineyard</td>\n",
       "      <td>4</td>\n",
       "      <td>0.451794</td>\n",
       "      <td>http://www.kolitha.com/hcdt/ProcessDigitalTwin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LTA component</td>\n",
       "      <td>3</td>\n",
       "      <td>0.386161</td>\n",
       "      <td>http://www.kolitha.com/hcdt/sosa:Sensor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label  freq  best_align  \\\n",
       "0                          UAV    11    0.407679   \n",
       "1             NVIDIA Isaac Sim     8    0.457461   \n",
       "2                         UAVs     7    0.416369   \n",
       "3              Data Middleware     5    0.457772   \n",
       "4                     UC-AGR-2     5    0.493644   \n",
       "5  DJI Phantom 4 Multispectral     4    0.422528   \n",
       "6                     UC-AGR-3     4    0.434973   \n",
       "7                     UC-AGR-1     4    0.450762   \n",
       "8                     vineyard     4    0.451794   \n",
       "9                LTA component     3    0.386161   \n",
       "\n",
       "                                       parent_iri  \n",
       "0         http://www.kolitha.com/hcdt/sosa:Sensor  \n",
       "1         http://www.kolitha.com/hcdt/sosa:Sensor  \n",
       "2               http://www.sws.org/sws/Ergonomics  \n",
       "3   http://www.kolitha.com/hcdt/DataVisualization  \n",
       "4             http://www.sws.org/sws/AIAcceptance  \n",
       "5   http://www.kolitha.com/hcdt/DataVisualization  \n",
       "6       http://www.kolitha.com/hcdt/NonHumanActor  \n",
       "7             http://www.sws.org/sws/AIAcceptance  \n",
       "8  http://www.kolitha.com/hcdt/ProcessDigitalTwin  \n",
       "9         http://www.kolitha.com/hcdt/sosa:Sensor  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def best_align_score(e): \n",
    "    a = align_entity_to_class(e, topk=3)\n",
    "    return a[0][\"score\"] if a else 0.0\n",
    "\n",
    "def discover_new_concepts(freq_min=NEW_CONCEPT_FREQ_MIN, score_max=ENTITY_ALIGN_MIN):\n",
    "    counter, ents_by_name = Counter(), {}\n",
    "    for h in HYPER_NODES.values():\n",
    "        for eid in [n for n in G.neighbors(h[\"id\"]) if G.nodes[n][\"kind\"]==\"entity\"]:\n",
    "            ent = ENTITY_NODES[eid]; key = (ent[\"name\"] or ent[\"type\"]).strip()\n",
    "            ents_by_name[key] = ent; counter[key]+=1\n",
    "    proposals=[]\n",
    "    for name, freq in counter.items():\n",
    "        e = ents_by_name[name]; s = best_align_score(e)\n",
    "        if freq >= freq_min and s < score_max:\n",
    "            qv = embed_query(name); nns = nearest(CLS_MAT, CLS_IDX, qv, k=1)\n",
    "            parent = CLASS_LABELS[nns[0][1]][\"iri\"] if nns else f\"{BASE}Entity\"\n",
    "            proposals.append({\"label\": name, \"freq\": freq, \"best_align\": s, \"parent_iri\": parent})\n",
    "    return sorted(proposals, key=lambda x:(-x[\"freq\"], x[\"best_align\"]))\n",
    "\n",
    "NEW_CONCEPTS = discover_new_concepts()\n",
    "pd.DataFrame(NEW_CONCEPTS)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "acfc008f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: D:\\hyper-rag-ontology\\outputs\\abox.ttl\n",
      "Wrote: D:\\hyper-rag-ontology\\outputs\\tbox.ttl\n",
      "Wrote: D:\\hyper-rag-ontology\\outputs\\new_concepts.csv\n"
     ]
    }
   ],
   "source": [
    "def export_updates_for_query(query:str, top_facts=10):\n",
    "    _,_,facts = fused_retrieval(query, K_ENTITY, K_HYPEREDGE)\n",
    "    facts = facts[:top_facts]\n",
    "    abox_all, tbox_all, new_rows = [], [], []\n",
    "    for f in facts:\n",
    "        ax = propose_axioms_for_fact(f)\n",
    "        if ax[\"abox\"]: abox_all.append(ax[\"abox\"])\n",
    "        if ax[\"tbox\"]: tbox_all.append(ax[\"tbox\"])\n",
    "        new_rows += ax[\"new_classes\"]\n",
    "\n",
    "    abox_ttl = \"@prefix : <http://example.org/onto#> .\\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n@prefix owl: <http://www.w3.org/2002/07/owl#> .\\n\\n\" + \"\\n\\n\".join(abox_all)\n",
    "    tbox_ttl = \"@prefix : <http://example.org/onto#> .\\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n@prefix owl: <http://www.w3.org/2002/07/owl#> .\\n\\n\" + \"\\n\\n\".join(sorted(set(tbox_all)))\n",
    "\n",
    "    (OUT_DIR / \"abox.ttl\").write_text(abox_ttl)\n",
    "    (OUT_DIR / \"tbox.ttl\").write_text(tbox_ttl)\n",
    "    pd.DataFrame(NEW_CONCEPTS).to_csv(OUT_DIR / \"new_concepts.csv\", index=False)\n",
    "    pd.DataFrame(new_rows).to_csv(OUT_DIR / \"new_classes_from_facts.csv\", index=False)\n",
    "    print(\"Wrote:\", (OUT_DIR/\"abox.ttl\").resolve())\n",
    "    print(\"Wrote:\", (OUT_DIR/\"tbox.ttl\").resolve())\n",
    "    print(\"Wrote:\", (OUT_DIR/\"new_concepts.csv\").resolve())\n",
    "\n",
    "export_updates_for_query(\"hypertension serum creatinine male mild elevation\", top_facts=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
