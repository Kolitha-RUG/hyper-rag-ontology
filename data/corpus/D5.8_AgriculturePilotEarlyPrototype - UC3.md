# Overview of the EP of the Agriculture Pilot

Viticulture in mountainous regions such as Portugal’s Douro Valley presents persistent challenges for automation, primarily due to steep terrain, heterogeneous grapevine varieties, and a continued reliance on manual labour. Vineyard operations—ranging from grape identification to harvest logistics—remain physically demanding and difficult to optimize through conventional means. Labour shortages and environmental variability further complicate operational planning and execution. These constraints are exemplified by the selected experimental site, Vinha Maria Teresa (VMT)—a 4.7-hectare centenary vineyard situated on the eastern slope of Quinta do Crasto S.A., at altitudes between 120 and 190 meters near the Douro River. VMT is traditionally non-mechanized, composed of wide terraces bordered by dry stone walls and planted with a stochastic mix of Vitis vinifera L. varieties. The vineyard employs short pruning and dense planting (0.80 m × 1.40 m spacing, ~7,941 vines/ha), with grapevines supported by horizontal wire trellises affixed to schist posts. This complex and heritage-rich environment necessitates tailored, AI-driven approaches to enable sustainable automation while respecting traditional practices and site-specific constraints (see Figure 1).

Figure - Orthophoto mosaic of the Vinha Maria Teresa (VMT) vineyard overlaid on satellite imagery.

Located on steep terrain along the eastern slope of Quinta do Crasto in the Douro Valley, VMT exemplifies the structural and operational challenges of traditional, non-mechanized viticulture in mountainous regions.

**Phenological Considerations in Data Acquisition**

Effective data acquisition in viticulture depends on aligning collection efforts with the grapevine’s phenological calendar. The month of June represents a critical stage in the lifecycle—typically corresponding to the beginning of berry set—when the critical phase of observation and analysis of the grapevines mature leaves also begins, according to the recommendations of the International Organisation of Vine and Wine (OIV), and which will continue until the veraison. This period also provides favourable conditions for data gathering, including stable weather and optimal lighting, which are particularly important in mountainous areas like the Douro Valley, where microclimates can affect vine development and data quality.

Within the scope of this deliverable, data collection was intentionally scheduled for June to capture this phenological window (see Figure 2). The timing was essential to ensure the relevance and quality of data used for downstream analysis, including the training and validation of AI components. As a result, the seasonal constraints directly shaped pilot preparation activities, requiring early coordination among technical teams, field access permissions, and environmental monitoring. Any deviation from this window would have risked collecting data that was either incomplete or misaligned with the vineyard’s biological cycle, thereby reducing the effectiveness of the pilot’s subsequent stages.

Figure 2 - Timeline and methods for grapevine varietal identification.

Morphological characterisation is performed at specific phenological stages using OIV descriptors, with the most discriminative leaf traits assessed between berry set and veraison.

The Agriculture Pilot addresses these challenges by introducing early-stage AI and robotics components into vineyard workflows. It focuses on three main use cases:

1. UC-AGR-1: Identification of grape varieties using image-based AI;
2. UC-AGR-2: Monitoring of vineyard conditions through UAVs and environmental sensors;
3. UC-AGR-3: Support for grape transport and fatigue-aware task distribution during harvest.

To enable early testing and validation, the EP was implemented in a controlled setting using simulation environments and partial data collection. The current implementation is simplified relative to the Final Prototype and includes:

- Use of UAV simulators (Pegasus and Isaac Sim) instead of continuous field deployments;
- Limited image datasets combined with synthetic imagery to train initial AI models;
- Initial Digital Twin prototypes based on static or semi-dynamic inputs;
- Rule-based task adjustment logic, with no real-time feedback from deployed sensors;
- Integration of pilot-specific components such as wearable fatigue data processing and terrain-aware flight logic.

These simplified setups allow for validation of technical feasibility and integration workflows without full operational complexity. Each use case is implemented using the core AI4Work architecture, adapted to fit the agricultural context.

The next phase will involve expanding the real-world data collection, increasing the realism of simulations, and improving the interaction between AI systems and human operators, particularly in the areas of task planning, Digital Twin evolution, and UAV coordination.

## Pilot specific Infrastructure

### UC-AGR-3: Transport of the grapes in a steep-slope vineyard during the harvest season

This use case addresses the physical burden associated with manual grape transport during harvest in steep-slope vineyards. The objective is to enhance operational efficiency and improve worker safety through the use of autonomous cargo drones and fatigue-aware task planning.

As part of the Early Prototype, a drone-based transport system is currently under design and simulation, aimed at supporting vineyard workers in transporting grape boxes across challenging terrain. The envisioned system incorporates wearable IoT sensors to track fatigue indicators such as physical exertion, repetitive movements, and environmental stress exposure (e.g., high temperatures).

Collected physiological data will inform future AI-driven scheduling logic, but this capability is not yet integrated in the current Early Prototype. Similarly, components such as Runtime Monitoring, Explainable What-If Analysis, and Sliding Work Sharing (SWS) are not implemented in this phase and are planned for integration in subsequent development cycles to support adaptive workload distribution and AI transparency.

The Early Prototype incorporates:

- Human-aware Task Planning, which guides the initial design of scheduling mechanisms for drone deployment, accounting for worker fatigue and terrain constraints;
- Context Awareness, used to model environmental conditions and operational factors relevant to drone navigation;
- A Digital Twin, which captures spatial, physiological, and operational data for scenario planning and future integration with vineyard logistics systems.

Although the full AI pipeline for task prioritization and drone dispatch remains under development, the current setup provides a strong foundation for integrating sensor-based monitoring with human-in-the-loop planning. Future iterations will enhance transparency by allowing workers to interpret and adjust AI recommendations, improving both safety and efficiency across prolonged harvest periods. See Figure 5 for the corresponding system diagram.

# Pilot-Specific installation, customisation and configuration of AI4Work technologies

As discussed earlier, several AI4Work technologies have been integrated and adapted to address the requirements of the Agriculture Pilot, with a focus on the steep-slope vineyard use case. The Early Prototype phase includes components supporting human-aware task planning, fatigue monitoring, digital twin development, and context-aware decision-making. In the following subsections, each relevant AI4Work technology is described. This section outlines core AI4Work components, with each subsection including the preparatory steps necessary for their adaptation and deployment in the specific operational setting of the vineyard.

## Human-aware Task Planning for human-machine interaction

In steep-slope vineyard environments, manual labour is physically demanding, with task intensity and environmental exposure contributing significantly to worker fatigue. To support safe and efficient collaboration between human workers and autonomous systems, the pilot implements human-aware task planning strategies. These strategies rely on real-time physiological data collected via wearable IoT sensors (see Figure 6), enabling the AI system to monitor worker status and adapt task allocation dynamically. This section describes the wearable sensor setup, monitored physiological variables, fatigue threshold definitions, and the role of this data in supporting decision-making between human and robotic agents.

Two types of wearable devices were selected for deployment. A chest-mounted sensor collects heart rate, ECG signals, body temperature, and GPS coordinates. An arm-mounted device captures motion data through an accelerometer and gyroscope. Together, these sensors provide a comprehensive view of the worker’s physiological state and movement profile during field operations.

Figure 6 - Wearable IoT sensor setups used for physiological monitoring.

Figure 6, top row: examples of chest, wrist, and thigh sensor placements on mannequins and humans. Bottom row: close-up views of sensor bands and fastening mechanisms designed for secure attachment and ease of wear during field deployment.

### GPS Tracking and Location Awareness

GPS data from the chest-mounted device is recorded continuously and available in real time. This enables precise localisation of workers during vineyard activities, supporting geofencing, dynamic task coordination, and rapid emergency response when required.

An example GPS track recorded during a session between 11:39 AM and 3:36 PM covered a total distance of 1.85 km, with an altitude variation of 30 meters (see Figure 7). This level of spatial and temporal resolution supports correlation with task intensity and terrain difficulty.

Figure 7 - Example GPS track of a vineyard worker recorded between 11:39 AM and 3:36 PM, covering a total distance of 1.85 km with an altitude variation of 30 meters.

### Body Temperature Monitoring

Body temperature was monitored over a session from 8:31 AM to 10:23 AM. The maximum temperature of 37.24°C was recorded approximately 20 minutes into the session, corresponding to a sustained period of physical activity. The minimum temperature of 36.15°C occurred during a short break, before rising again as work resumed (see Figure 8). These patterns help identify workload phases and physiological recovery dynamics.

Figure 8 - Body temperature readings over time during fieldwork on 2025-06-27. The recorded values ranged from a minimum of 36.15 °C to a peak of 37.24 °C, reflecting physiological responses to task intensity and environmental conditions.

### Heart Rate Monitoring

Heart rate data, recorded over the same session, followed a similar pattern to body temperature. The peak heart rate of 138.21 BPM occurred at 8:52 AM, coinciding with the highest temperature reading. The minimum heart rate of 77.09 BPM occurred during the rest period at 9:32 AM. These results confirm the close relationship between cardiovascular response and work intensity (see Figure 9).

Figure 9 - Heart rate measurements recorded during vineyard fieldwork on 2025-06-27. The values ranged from a minimum of 77.09 BPM to a peak of 138.21 BPM, indicating periods of varying physical effort.

### Fatigue Thresholds

To support automated task redistribution, a set of fatigue thresholds is being defined based on literature and preliminary data. For example, body temperature exceeding 38°C and heart rate sustained above 140 BPM for more than 10 minutes are associated with increased perceived fatigue and reduced performance. Additionally, the Rating of Perceived Exertion (RPE) scale is under evaluation as a subjective fatigue metric, where values of 15 or higher typically indicate a need for rest in high-intensity work environments.

These thresholds inform the AI system’s logic for task balancing between human workers and UAVs, enabling workload adaptation in response to individual fatigue levels. This contributes to safer working conditions and more sustainable field operations without compromising task continuity.

## Long-term adaptation for reliability under uncertainty

The Long-Term Adaptation (LTA) component was specialized for the agricultural pilot in a tool-supported implementation called Adaptive Robot-human Collaboration with Hybrid planning (**ARCH**) \[1\]. ARCH implementation is shown in Figure 11. More details about this approach can be found in D3.1.

Figure 10 -Long-Term Adaptation component for the agriculture pilot.

ARCH receives a probabilistic robot-human collaboration planning problem along with mission constraints and optimization objectives; it then generates a verified plan and corresponding quality-of-service metrics. The customization for the agricultural pilot is demonstrate in a scenario summarised as follows:

- The vineyard is divided into subsections where different varieties of vines grow.
- There are three types of possible task:
  - t1: harvesting of the plants,
  - t2: vineyard monitoring activities,
  - t3: grapevine identification, i.e., collecting images of the vines.
- There exists a set of agents (human workers and robots) available to complete the tasks.
- Each agent has a different probability of succeeding with their tasks.
- Each agent has different associated costs (for example, humans' cost can consider a combination of fatigue levels and travelling distance, while robots’ costs model the battery consumption and travelling distance).
- There is a maximum number of times agents can retry their tasks.
- Two constraints can be defined by the user:
  - minimum probability of succeeding
  - the (minimum probability) threshold to assign tasks to agents (i.e., if agents cannot perform a task with a certain probability of success, they cannot be allocated with such task).

An instance of the agent’s capabilities is depicted in Table 2. In this example, humans can perform tasks t3 with a cost of 5 units and succeed on average 99% of the time; if they fail, they are allowed to retry up to 5 times before considered a failure.

Listing 1 - Input file inputLTA.json

Internally, ARCH produces a sequence of intermediate outputs throughout its processing pipeline:

- A planning problem encoded in the **Planning Domain Definition Language (PDDL)**;
- A deterministic plan generated using off-the-shelf PDDL planner solvers;
- An optimisation problem formulated to determine the most effective combination of task retries, aiming to maximise mission success probability while minimising associated costs. This problem is expressed using the **EvoChecker** language, comprising: a .pm file defining the probabilistic model, and a .props file specifying the formalised requirements;
- A set of Pareto-optimal retry configurations per task.

To enable integration with downstream modules within the AI4Work architecture, the generated plan and verification results are aggregated and reformatted into a unified JSON output. An example of this output is presented in Listing 2. First, a plan is described as a sequence of actions: move to another location or do a task. Each action has an assigned agent, a location, an end location if the robot is travelling, and the task it must perform while doing such action. Second, a Pareto optimal list of solutions is defined. Each object contains a possible combination of retries permitted for each task, and the quality of such solution. For example, in the first Pareto solution ("paretoSolutionid": 1), task t1l7 can be retried up to 2 times by worker2; this solution results in an expected cost of 37.731 units and succeeds with 0.961 probability.

Listing 2 - Output file outputLTA.json

**Adaptation:** The LTA component is initialised when an adaptation trigger is received. ARCH generates a verified plan, and different retry-augmented plans with their respective quality of service values. When a change is detected at runtime, this component triggers an efficient adaptation process taking into account previously generated plans from which a new plan can be selected (if compliant with the new problem definition after the change). Four changes are monitored for the early prototype:

- _C1_: a task failure (e.g., updated by the digital twin AI4Work component);
- _C2_: a change in the minimum success probability (e.g., updated by the context-aware AI4Work component);
- _C3_: a change in the minimum assignment probability (e.g., updated by the context-aware AI4Work component);
- _C4_: a change in an agent’s probability to complete a task (e.g., updated by the digital twin AI4Work component).

Our component generates a new plan, if it exists, that accommodates these changes as they happen at runtime. It uses an efficient adaptation algorithm to select and execute the step within ARCH where the change impacts the generation of feasible plans, to avoid computational expenses when necessary. Further details are described in D3.1.

**Conclusions:** The LTA early prototype was tested on a preliminary description of the agricultural case study. It provides a structure for the description of the planning problem accommodating specifics of the agricultural tasks, such as collaborative tasks between humans and robots for monitoring tasks across different locations of the vineyard, harvesting of grapes and monitoring tasks such as monitoring weather conditions or collecting images of the vines. Our LTA component can accommodate different capabilities for the different involved agents. Finally, we describe the inputs and outputs of ARCH, our specialised tool-supported framework, and changes monitored at runtime for the generation of a new plan.

## Human-Centred Digital Twins of processes and employees

To support AI4Work’s vision of safe, efficient human-AI collaboration in labour-intensive agricultural environments, this section focuses on the initial steps taken to implement Human-Centred Digital Twins (HCDTs). As outlined in the conceptual framework defined in Deliverable D1.2, these digital replicas serve two primary functions: (i) modelling operational processes such as drone-based transport in steep-slope vineyards, and (ii) mirroring worker status, environmental factors, and terrain complexity to support adaptive, context-aware planning.

In the specific context of the UC-AGR-2 pilot, the VMT exemplifies a challenging setting due to its steep slopes, variable accessibility, mixed varietal planting, and vulnerability to critical infrastructure events (e.g., erosion, storm damage). These conditions informed the identification of high-priority zones for monitoring and intervention, guiding both fieldwork planning and the requirements for digital representation.

The subsections that follow are organised into three complementary efforts. Section 3.4.1 identifies critical vineyard zones and their operational relevance. Section 3.4.2 details the acquisition of grapevine-specific metadata—such as varietal ID, GPS coordinates, and image conditions—supporting integration with the Digital Twin structure. Finally, Section 3.4.3 introduces the initial virtual prototype of the vineyard environment using NVIDIA Isaac Sim, PX4, and Pegasus. This early-stage implementation allows simulation and validation of drone-based tasks, laying the foundation for AI training, adaptive mission logic, and future integration of worker behaviour models.

### Early-Stage Digital Twin Implementation for Vineyard Drone Operations

To support the development of a Digital Twin for drone-based grape transport scenarios, a virtual testing environment was built using NVIDIA Isaac Sim \[3\], the PX4 Autopilot \[7\], and the Pegasus Simulator \[8\] This setup enables comprehensive simulation of drone operations, allowing testing and validation of behaviors in a photorealistic 3D world, without relying on physical drone hardware. The following diagram (see Figure 19) illustrates the full simulation architecture and the data flow between each component in the system.

Figure 19 - Architecture of the virtual drone simulation system

These simulated scenarios allow for the testing of complex task conditions—such as varied terrain, dynamically positioned workers, environmental changes, and multi-drone coordination. This capability underpins the training and validation of AI4Work tools, particularly those targeting adaptive behaviour, real-time task reassignment, and human-aware interaction strategies. In future iterations, simulated worker agents will incorporate models of fatigue and performance variation based on contextual factors like slope, heat, and task repetition. These models will directly affect parameters such as movement speed, task completion rates, and responsiveness within the simulated environment. Their impact on overall task efficiency, workload balancing, and safety-aware planning will also be evaluated. This will enable more realistic scenario planning and allow stress-testing of Long-Term Adaptation module. More detailed information on the physiological modelling of workers is available in other sections of this deliverable.

Pegasus, built on NVIDIA Isaac Sim, leverages its advanced physics engine, high-fidelity rendering, and capability to simulate heterogeneous agents—including drones, human workers, and static vineyard infrastructure. This enables realistic and immersive mission validation, reducing development time and helping to bridge the Sim2Real gap by aligning simulated behaviour with real-world conditions (see Figure 20). While the current prototype does not yet fully replicate ground vegetation, upcoming data collection campaigns in the next growing season will be used to refine the simulation. This feedback loop—linking field data with simulation assets—will support progressive updates to visual realism and environmental fidelity in future simulation iterations.

Figure 20 - Visual comparison between synthetic and real vineyard imagery.

Figure 20 Top: Rendered image from the simulated environment in Isaac Sim, used for testing UAV perception and mission logic. Bottom: Real-world aerial image captured from an operational vineyard. This comparison illustrates the photorealism achieved in simulation, supporting Sim2Real transfer and AI model training.

At the core of the current simulation environment, PX4 runs in Software-In-The-Loop (SITL) mode. PX4 is an open-source flight control software compatible with multiple UAV platforms, making it a flexible starting point for simulation and integration. In this setup, PX4 handles real-time flight dynamics and vehicle control, interpreting mission commands, processing sensor inputs, and producing actuator outputs. Communication is conducted using the MAVLink protocol, which allows PX4 to transmit telemetry data and receive mission instructions.

To mirror the expected integration with real UAVs, the simulation environment also incorporates a DJI ROS driver that emulates the DJI UAV interface. While this distinction is abstracted during simulation runtime, it plays a key role in developing and validating a platform-agnostic integration architecture. This abstraction layer allows the system to support multiple UAV types—both DJI and PX4-based platforms—by isolating UAV-specific communication logic from the core Digital Twin and mission control logic. The DJI ROS interface is currently under active development to ensure compatibility with the selected UAV platforms and to support seamless transition from simulation to real-world deployment. It is important to note that the objective of this simulation environment is not to replicate low-level control loops (e.g., PID tuning, motor dynamics), but rather to validate task-level behaviour, mission coordination, and integration workflows. This level of abstraction is sufficient and appropriate for testing the AI4Work platform’s planning and Digital Twin components. Pegasus acts as a middleware between PX4 and Isaac Sim, translating telemetry from PX4 into real-time updates in the Isaac Sim world and feeding back synthetic sensor data such as GPS and IMU readings to PX4 in MAVLink format. Isaac Sim handles rendering of terrain, objects, and flight physics, generating a realistic operational context.

A Ground Control Station (QGroundControl) is used to manage UAV missions as it would in a real deployment. It connects to PX4 to define flight plans, monitor telemetry, and issue commands during the mission.

The diagram Figure 19 in illustrates the full simulation architecture. Figure 21 presents the step-by-step process for running a virtual flight mission using this setup.

Although Isaac Sim supports simulation of complex dynamic agents, such as humans, the current early prototype does not yet include fully simulated workers. Preliminary integration work is underway, with the goal of enabling simulation of worker positions and activities within the virtual vineyard. This will be essential for supporting human-aware long-term adaptation and sliding-work-sharing described in Sections 3.2 and 3.6. Future development will focus on dynamically modelling worker movement and behaviour, enabling robust testing of UAV response strategies under real-world uncertainty and proximity constraints.

This staged approach allows the project to gradually introduce more complex scenarios, supporting long-term adaptation for safety, reliability, and efficiency in human-machine collaboration.

Figure 21 - Step-by-step virtual drone simulation workflow

**Simulated Environment Construction for UAV Mission Validation**

To realistically simulate UAV operations in vineyard environments, the virtual testbed is configured using NVIDIA Isaac Sim, augmented with custom-built terrain and asset models replicating the real-world layout of VMT.

Figure 22 - End-to-End Workflow for Vineyard Simulation Environment Creation.

The vineyard environment is generated through a structured multi-step workflow (Figure 22), designed to faithfully reproduce the terrain, row structure, and vegetation density observed in the field:

- Image Acquisition: Aerial images of the vineyard are captured using drone-mounted cameras.
- Terrain Modeling (WebODM): These images are processed into a textured 3D terrain model using WebODM photogrammetry tools (see Figure 23).
- Mesh Flattening (Blender): The resulting terrain mesh is refined and flattened where needed using Blender to ensure compatibility with simulation constraints.
- 3D Model Import: Rows of vines are modeled or selected from a 3D asset library and imported into the simulation.
- CSV-Based Placement: The placement and spacing of vines are controlled using CSV-based layout definitions, enabling reproducibility and parametric editing.
- Final Scene Assembly: The final simulation scene is compiled and optimized inside Isaac Sim.

Figure 23 - Photogrammetric Terrain Reconstruction of Maria Teresa vineyard.

Figure 23 shows high-resolution 3D terrain model generated via WebODM from aerial drone imagery. This mesh forms the geometric basis for the simulation environment (referenced in Terrain Modelling step above).

This pipeline ensures that the geometry and row spacing of the virtual vineyard closely match those of VMT, including typical intra-row spacing of ~80 cm and a maximum of 1.2 meters in perpendicular walking paths. Terrain variations and asset layout are deliberately retained to challenge drone navigation and support realistic testing of human-drone interaction.

The simulation environment is created inside NVIDIA Isaac Sim. There are two main ways to configure the scene:

- **A. Using the Pegasus Simulator Extension UI**: The user selects a vineyard from the available assets, sets the GPS coordinates and vehicle model, and positions the drone in the scene. After clicking **Play**, the simulation begins.

_(Note: At the time of this setup, the Extension UI does not support configuring cameras directly.)_

- **B. Using an Automated Script:** This option replicates all the steps mentioned above but also automatically begins capturing images using the simulated drone camera. It is typically more convenient and ensures consistency across runs.

Figure 24 shows the simulated vineyard environment created in Isaac Sim. The virtual landscape is populated with 3D assets to replicate realistic conditions for drone flight.

Figure 24 - Simulated Vineyard Environment

**Mission Control Workflow**

To realistically simulate drone operations aligned with the UC-AGR-2 pilot, flight missions must follow real-world planning and control workflows. This ensures that the Digital Twin not only reflects the physical environment but also mirrors operational logic and control protocols used in vineyard logistics. The use of QGroundControl (QGC), PX4, and MAVLink allows for precise mission design, telemetry validation, and autonomy testing under vineyard-specific conditions, supporting both technical verification and future deployment alignment.

The virtual drone is controlled using QGroundControl (QGC), an open-source ground control station that communicates with PX4 via the MAVLink protocol. This setup mirrors real-world UAV operations, allowing mission plans created for actual hardware to be directly tested in simulation with minimal modification. QGroundControl provides an intuitive graphical interface for planning missions, configuring UAV behaviour, and monitoring flight telemetry in real time.

The typical workflow for flight controller setup in the simulation environment includes:

- Launching the QGroundControl application: The operator starts QGC on the host machine, which automatically establishes a connection to the PX4 SITL instance running as part of the simulation.
- Creating or importing a predefined mission plan: A sequence of waypoints, actions (e.g., image capture, loiter), or survey patterns is defined within QGC’s mission planning interface. For repeatable testing, predefined .plan files are imported and adjusted programmatically or through scripted processes as needed.
- Uploading the mission to the virtual vehicle: Once the plan is complete, it is uploaded to the PX4 instance via MAVLink. QGroundControl verifies the integrity of the mission and provides feedback on any potential configuration issues (e.g., altitude inconsistencies or waypoint spacing).
- Verifying simulation conditions: Before starting the mission, the operator confirms that GPS lock, battery status (simulated), and flight mode are correctly initialized in QGC. This step helps ensure valid mission execution in the virtual environment.
- Starting the mission: The mission is initiated from QGroundControl, prompting PX4 to begin autonomous execution of the uploaded instructions. QGC can be used to monitor the drone’s position, speed, attitude, and live mission status throughout the flight.

This control setup allows the same mission logic that would be used in real-world UAV deployment to be tested in a photorealistic and physics-accurate environment. It enables rapid iteration and validation of flight plans, helping ensure safety, reliability, and effectiveness before physical deployment.

Once started, the drone autonomously follows the uploaded mission. It moves through the simulated vineyard, collecting telemetry and simulated sensor data in real time. If camera simulation is active, it also captures visual data from the virtual environment.

After completing the mission, both Isaac Sim and QGroundControl are closed to end the session. The data collected during the run—such as images from the onboard camera or telemetry logs can then be reviewed for analysis or validation. The figure below shows sample frames captured by the simulated drone’s camera during its mission.

Figure 25 -– Sample frames captured by the UAV’s camera

## Context Awareness

The Context Awareness component of AI4Work supports dynamic adaptation of AI-driven systems by integrating environmental, spatial, and operational information into real-time decision-making. In the context of the UC-AGR pilots—and particularly the steep, terraced terrain of Quinta do Crasto—this capability is essential for enabling UAVs to navigate safely, collect high-quality data, and respond appropriately to mission-critical events.

This section presents the preparatory work conducted to define and test context-aware UAV behaviour ahead of full pilot deployment. These efforts include the identification of critical monitoring zones (see also Subsection 3.4.1), the definition of flight parameters aligned with environmental and operational constraints, and the integration of terrain and geospatial data into simulation-based validation workflows.

All configurations will be evaluated using the NVIDIA Omniverse-based simulation environment developed within AI4Work, which replicates the vineyard’s physical structure and supports iterative refinement of mission planning and UAV response strategies. These foundational steps lay the groundwork for future integration of more advanced context modelling mechanisms in the next phases of the project.

### Monitoring Route Objectives

Monitoring flights are configured to collect structured datasets aligned with the goals of UC-AGR-1 and UC-AGR-2. Routes are built around the following operational goals:

- Pest and Disease Monitoring Potential: UAVs fly low-altitude paths to collect leaf-level imagery. These images are intended for potential use in developing AI segmentation and detection pipelines. Mission plans target specific zones identified in Subsection 3.4.1 as having higher disease sensitivity due to vine variety composition and prior observations. Monitoring of Critical Vineyard Zones: UAV paths revisit key areas in VMT:
  - The slope near the access road, with observed vine mortality and structural risk;
  - Terraces between 120 and 140 meters of elevation, noted for limited accessibility and maintenance challenges;
  - Mixed-variety sectors, where disease-prone and resistant grapevines coexist, requiring more targeted monitoring.
- Microclimate Characterization: UAVs collect environmental data along predefined routes following topographic gradients (elevation, exposure). These missions are coordinated with ground-based weather station data collection. The weather stations currently being deployed at Quinta do Crasto provide contextual environmental data to correlate with UAV sensor readings.

Each mission is repeatable and spatially consistent, ensuring alignment with long-term monitoring strategies and compatibility with AI-based decision support tools.

### Altitude Management and Obstacle Avoidance

Given the terraced structure and narrow rows in the vineyard (see Figure 27), precise control over UAV positioning is required. The following initial flight control strategies are currently under development and subject to testing:

- Minimum Flight Altitude: The minimum altitude is under evaluation through simulation and field testing (June 2025). A reference clearance of 2.5 m above canopy is used as a baseline, but the final rows of each vineyard plot often require the UAV to ascend further to avoid vines from adjacent upper terraces. Worker presence on neighbouring steps is also a factor. As such, a dynamic strategy is necessary to modulate UAV altitude relative to both terrain and worker positions, while ensuring sensors remain within their optimal operating envelope.
- Terrain Following: UAVs use terrain-following logic based on Digital Surface Models (DSM) and Digital Terrain Models (DTM) to dynamically adjust altitude along sloped vineyard plots. This ensures consistent canopy-relative altitude even in areas with sharp elevation changes between terraces. Maintaining this alignment is critical for capturing datasets with consistent resolution and ground sampling distance (GSD), particularly when collecting imagery for analysis over multiple time points.
- Obstacle Awareness and Avoidance: The simulation includes relevant obstacles such as trellis systems, poles, and potential worker locations. UAV behaviours are tested in simulation to confirm proper reaction to near-field objects.

Descent and Ascent Rate Limits: Rapid changes in altitude across terraces are rate-limited to ensure platform stability.

### Environmental Operating Conditions

Environmental constraints are based on UAV specifications and are being gradually integrated into simulation:

- Wind Tolerance: Nominal flight operations are tested up to 8 m/s wind speeds. Performance is assessed separately for each UAV platform.
- Fog and Visibility: UAV-based navigation and data capture require minimum visibility of 30 meters. Reduced visibility affects image quality and obstacle detection.
- Temperature Limits: Flight plans consider environmental conditions in accordance with manufacturer operating ranges, particularly for the Matrice 210 RTK and Phantom 4 Multispectral that are now being used.

These environmental parameters are being modeled in simulation with a level of fidelity suitable to AI4Work objectives. While not focused on high-precision physical simulation, this integration supports testing the robustness of mission planning and adaptation strategies.

**Simulation-Based Validation**

All flight conditions will be tested in the Isaac Sim-based Omniverse simulation environment, which replicates the physical structure of Quinta do Crasto. This environment enables:

- Systematic validation of terrain-aware flight paths;
- Integration testing of UAV mission uploads and telemetry capture;
- Controlled trials of UAV behavior in constrained and degraded scenarios (e.g., high wind, low visibility);
- Safety assessments for proximity to infrastructure and dynamic agents.
- This testbed supports rapid iteration and risk-free evaluation of flight logic, aiding in the refinement of constraints before real-world deployment.

The defined flight conditions and constraints in this section also support the configuration of the Long-Term Adaptation (LTA) component described in Section 3.2. Specifically, parameters such as safe altitude margins, terrain-following strategies, and UAV performance under environmental constraints are used to inform the ARCH planning model. Simulated task failures or degraded conditions (e.g., wind thresholds or sensor blind spots) can act as triggers for runtime adaptation. These simulations contribute to building robust task allocation strategies across agents—both UAVs and human workers—by supporting realistic input generation and scenario-based evaluation within the AI4Work framework. While the LTA module does not yet incorporate these constraints in the current early prototype, this preparatory work ensures that the necessary parameters and test scenarios are in place for future integration and validation.

To enable consistent UAV-based monitoring, the critical locations identified in subsection 3.4.1 were georeferenced and will be integrated into a central monitoring and mission control system. This integration includes:

- Mapping each critical region with precise GPS coordinates;
- Storing this information in a spatial database accessible by the flight planning tools;
- Using Digital Surface Models (DSM) and Digital Terrain Models (DTM) (see Figure 26), derived from photogrammetry (WebODM), to associate elevation profiles with each location.

Figure 26 - Quinta to Crasto Digital Surface Model (DSM)

This spatial integration allows the UAV mission planner to adaptively tailor each mission to the topography of Quinta do Crasto, where vineyard terraces are carved into steep hillsides with substantial elevation differences over short distances (see Figure 27). These conditions introduce critical constraints on UAV stability, sensor accuracy, and data quality.

Figure 27 - View of point cloud the sloped terrain and steps of Quinta do Crasto. High-resolution point cloud visualization of Quinta do Crasto vineyard terraces.

Specifically, the integration enables the system to:

- Maintain Safe and Consistent Altitude: Flight paths are optimized to follow the underlying terrain, preserving a consistent vertical distance from the ground. This is essential for avoiding collisions and ensuring sensor stability when flying close to vegetation or along terraced rows.
- Enable Accurate Leaf-Level Image Capture: For tasks such as grape variety identification or disease detection, UAVs must maintain a stable and repeatable low-altitude flight over the vines. Capturing leaves at a consistent distance ensures that image resolution and perspective remain suitable for AI-based segmentation and analysis. On steep terrain, this requires precise altitude modulation using terrain-following logic.
- Adapt to Environmental Conditions: UAV trajectories can be updated in real time to account for wind, fog, or temperature shifts, which are more pronounced on slopes due to microclimatic variations and airflow disruption.
- Ensure Imaging Quality Across Elevation Changes: Orthomosaic generation and NDVI mapping rely on maintaining proper image overlap and consistent ground sampling distance (GSD). Without terrain-aware flight paths, these quality standards would degrade significantly over sloped plots.

In summary, steep terrain makes terrain-following UAV path planning essential, especially when operating at low altitudes for high-resolution visual tasks. This integration of geospatial data, elevation models (DSM/DTM), and environmental inputs ensures that UAV missions at Quinta do Crasto remain safe, efficient, and analytically valid.

A key part of the pipeline is the connection to QDC’s internal vine inventory platform, which includes a CSV-based registry of each vine’s location, ID, and variety (see Figure 28). This registry feeds directly into the simulation environment (see 3.4.3), where it guides the placement of synthetic vine assets and drone flight path validation within NVIDIA Isaac Sim.

Figure 28 - Quinta do Crasto's vineyard management interface

In the long term, these GPS-tagged zones will also support adaptive rescheduling or re-planning within the AI4Work platform, as new data or critical infrastructure events emerge.

## Sliding Work Sharing

The Sliding Work Sharing (SWS) Management component was developed as a generic software module that can be customized to different pilot scenarios via configuration files. More details about its general approach are presented in D4.1 Early Prototype of Building Blocks.

The SWS component was not yet integrated into the EP architecture, but an initial application example was developed, demonstrating how the SWS component can be applied to support the use case UC-AGR-3: Transport of the grapes in a steep-slope vineyard during the harvest season. The application scenario can be summarized as follows:

- workers in the vineyard are harvesting grapes and putting them into boxes
- whenever a box is full of grapes, it must be carried to a central collection point
- this carrying process can be done either by the workers themselves or by a transport drone
- the transport drone has limited carrying capacity, so it cannot transport all boxes for all workers

The SWS Management component should support the decision whether a specific transport should be done by the drone or by the worker and in how far a human (either worker or supervisor) should be involved in the decision. The following variables are considered:

- the current waiting time for the drone
- the fatigue level of the worker
- the distance from the current location of the box to the central collection point

To configure the SWS Management component for this scenario, a custom FCL file has to be created, which defines the above-mentioned input parameters, the potential outputs, the membership functions for fuzzification and defuzzification as well as the fuzzy decision rules. An example of such an FCL file is shown in the following Listing 3.

FUNCTION_BLOCK agricultureTransportSlidingDecisionRules

VAR_INPUT

distanceToCentralCollectionPoint : REAL;

waitingTimeForDrone : REAL;

fatigueLevelOfWorker : REAL;

END_VAR

VAR_OUTPUT

suggestedWorkSharingApproach : REAL;

END_VAR

FUZZIFY distanceToCentralCollectionPoint // in meters

TERM LOW := (1, 1) (90, 1) (210, 0);

TERM HIGH := (90, 0) (210, 1) (300, 1);

END_FUZZIFY

FUZZIFY waitingTimeForDrone

TERM LOW := (0, 1) (4, 1) (10, 0); // in minutes

TERM HIGH := (4, 0) (10, 1) (15, 1);

END_FUZZIFY

FUZZIFY fatigueLevelOfWorker // in percent

TERM LOW := (1, 1) (30, 1) (70, 0);

TERM HIGH := (30, 0) (70, 1) (100, 1);

END_FUZZIFY

DEFUZZIFY suggestedWorkSharingApproach

TERM HUMAN_MANUALLY := (0,1) (1,1) (1,0); // let the worker carry the box

TERM AI_AUTONOMOUSLY := (1,0) (1,1) (2,1) (2,0); // let the drone carry the box

TERM HUMAN_IN_THE_LOOP := (2,0) (2,1) (3,1) (3,0); // let the worker decide

TERM HUMAN_ON_THE_LOOP := (3,0) (3,1) (4,1) (4,0); // inform supervisor

METHOD : COG; // Use 'Center Of Gravity' defuzzification method

DEFAULT := 0; // Default is HUMAN_MANUALLY if no rule activates defuzzifier

END_DEFUZZIFY

RULEBLOCK No1

AND : MIN;

RULE 1 : IF distanceToCentralCollectionPoint IS LOW  
THEN suggestedWorkSharingApproach IS HUMAN_MANUALLY;

RULE 2 : IF distanceToCentralCollectionPoint IS HIGH OR waitingTimeForDrone IS LOW  
THEN suggestedWorkSharingApproach IS AI_AUTONOMOUSLY;

RULE 3 : IF waitingTimeForDrone IS HIGH AND fatigueLevelOfWorker IS LOW  
THEN suggestedWorkSharingApproach IS HUMAN_MANUALLY;

RULE 4 : IF waitingTimeForDrone IS HIGH  
AND distanceToCentralCollectionPoint IS HIGH  
AND fatigueLevelOfWorker IS LOW  
THEN suggestedWorkSharingApproach IS HUMAN_IN_THE_LOOP;

RULE 5 : IF waitingTimeForDrone IS HIGH AND fatigueLevelOfWorker IS HIGH  
THEN suggestedWorkSharingApproach IS HUMAN_ON_THE_LOOP;

END_RULEBLOCK

END_FUNCTION_BLOCK



## AI4Work Services Platform

Datasets collect by drone flights are quite large. These datasets usually aggregate multiple types of data, ranging from flight information including the flight path, and the data collected by the multiple sensors and cameras attached. Those datasets need to be stored in such a way eases the assess to them, both to persons and to tools that are needed to process the collected dataset to infer new information. The AI4Work Services platform can support this activity thought the Data Middleware.

The Data Middleware is an auxiliary component of the AI4Work Service Platform, dedicated at support the integration between data sources and AI4Work technologies. Data Middleware achieves it by providing a well-defined API supporting methods for adding, discovering, and retrieving datasets. For a efficient data discovery process, the Data Middleware must support the enrichment of data sets with metadata describing it, describing relevant properties such as the type of information that can be found within the dataset, information about the collection of data (e.g.: when as collected, here was collected, who collected, which equipment was used to collect), and describing the time interval of the information within the dataset.

In the current version of the Data Middleware, datasets are added thru a user interface. This interface allows users add new datasets and provide any relevant metadata and provides facilities to upload the files of the dataset. In Figure 29 is shown an example to the upload of a dataset composed by multiple files to the dataset “101 FPLAN”.

Figure 29 – Interface for uploading datasets

After the creating a dataset and uploading the corresponding data, users can navigate through the list of datasets and inspect the details of each one. In Figure 30 and Figure 31 are shown the details pages of 2 datasets corresponding to different drone flights. In each detail page, information is shown about the location of the data, or as this dataset corresponds to a drone flight, is shown the flight path automatically extracted from the .MRK file within the dataset; a timeline indicating the time interval corresponding to the collection of the dataset (i.e. time of the flight); and a list with the files that compose the dataset. From this list the user can explore the content of the dataset and download the files that are relevant to him.



## Pilot Specific developments

In the AI4Work Agriculture Pilot, Unmanned Aerial Vehicles (UAVs) play a central role in addressing the operational challenges of steep-slope vineyards. From monitoring crop health to transporting harvested grapes, UAVs are being evaluated as versatile tools that can enhance both productivity and worker safety. This section details the pilot-specific developments undertaken to assess and configure UAV platforms suited to the varied terrain and task requirements of vineyards such as VMT. These developments reflect both field-tested trials and simulation-based validation efforts, supporting a hardware-agnostic integration strategy that aligns with AI4Work’s long-term vision of adaptive and intelligent automation in constrained agricultural environments.

### Evaluation of UAV Systems for Vineyard Surveying and Cargo Tasks

In the context of the AI4Work Agriculture Pilot, Unmanned Aerial Vehicles (UAVs) are key enablers for automating and optimizing vineyard operations across multiple use cases, ranging from data collection for grape variety identification to the physical transport of grapes during harvest. Due to the distinct technical and operational demands of these tasks, a heterogeneous UAV setup was adopted.

Figure 32 - Aerial View of Maria Teresa Vineyard Showing Sloped Terraced Layout and Narrow Row Spacing.

One UAV is lightweight and agile, optimized for survey missions and high-frequency data collection, while the other is more robust and capable of performing load-carrying tasks in steep and irregular terrain. In addition to the steepness (see Figure 32) of the slopes, the narrow spacing between vines in traditional vineyards (typically not exceeding 1.2 meters) places physical constraints on the UAV platform dimensions (see Figure 33). Only smaller UAVs can safely navigate these areas without risking collisions, plant damage, or operating uncomfortably close to human workers.

Figure 33 - Matrice 210 RTK about to land on the widest part of the Maria Teresa Vineyard during a data gathering field trial.

This is particularly important during harvesting tasks where UAVs may need to approach or land near field workers to pick up grape loads. In such confined conditions, the presence of larger UAV’s can lead to safety concerns and operational inefficiencies, including disruption to manual workflows or increased discomfort due to proximity.

These spatial constraints are further compounded by the European regulatory framework governing UAV operations. According to Implementing Regulation (EU) 2019/947 and Delegated Regulation (EU) 2019/945, UAVs are classified into Open, Specific, and Certified categories. Platforms above 25 kg fall into the Certified category, requiring full airworthiness certification, pilot licensing, and design approval comparable to manned aircraft. Even in the Open category, UAVs exceeding 4 kg must maintain minimum separation from people and buildings, limiting their usability in terraced or densely planted vineyards. Additionally, operations such as Beyond Visual Line of Sight (BVLOS) or flights over uninvolved persons require specific authorisations under the Specific category, introducing further barriers to deployment

As a result, high-payload UAVs like the DJI FlyCart—measuring 2800 × 3085 × 947 mm—are currently unsuitable for deployment within this project. Their large physical footprint makes them impractical for narrow vineyard rows and terraced terrain, and their classification under EU regulations imposes strict certification requirements. However, mid-sized platforms capable of carrying 6–8 kg, while maintaining a compact form factor, may become viable as regulatory conditions evolve. The AI4Work mission control and simulation framework is built to support this forward-looking evaluation by enabling safe, risk-free virtual trials prior to committing to new hardware solutions.

This combination supports both the environmental monitoring and AI-based classification required in UC-AGR-1 and UC-AGR-2, and the physical transport and worker support needed for UC-AGR-3. The AI4Work platform itself is hardware agnostic, meaning it can integrate with different UAV systems depending on what best fits the specific operational context. This flexibility allows the platform to adapt to evolving mission requirements and supports deployment in varied vineyard environments. As the project progresses, there is also the possibility of testing and exploiting new UAV platforms that may become available, allowing the consortium to evaluate advances in size, payload, and autonomy for future integration. These UAVs were selected based on Unparallel’s prior experience using DJI platforms in agricultural contexts. While more advanced models exist, this choice balances technological capability with availability and operational readiness.

**UAV Platforms Overview**

Two UAVs initially selected to fulfil the complementary needs of the agricultural pilot:

- DJI Matrice 210 Real-Time Kinematic (RTK) V1 (can be equipped for instance with Micasense Altum): Industrial-grade quadcopter for advanced imaging and load-carrying operations.
- DJI Phantom 4 Multispectral: Lightweight UAV optimized for precise vineyard surveying and image-based classification.

Table 3 - Technical Specifications Summary

| Specification | DJI Matrice 210 Real-Time Kinematic (RTK) V1 | DJI Phantom 4 Multispectral |
| --- | --- | --- |
| Dimensions | 887 × 880 × 378 mm (unfolded) | Diagonal 350 mm |
| Weight | 4.57 kg (with TB55 batteries) | 1.49 kg |
| Max Takeoff Weight | 6.14 kg | 1.49 kg |
| Max Flight Time | 38 min (no payload) | 27 min |
| Operating Temp. | \-20°C to 45°C | 0°C to 40°C |
| Real-Time Kinematic (RTK) | External D-RTK system | Integrated RTK module |
| Max Payload | 1.57 kg | Fixed payload |
| Multispectral | Micasense Altum (blue, green, red, red edge, NIR + thermal) | 5-band multispectral sensor (blue, green, red, red edge, NIR) + RGB |
| Thermal Imaging | Yes | No  |
| Vision Systems | Forward and downward vision systems | Multi-directional obstacle sensing |
| Obstacle Avoidance | Yes | Yes |
| Transmission | 7 km range via Lightbridge | 7 km range via OcuSync |
| Battery | Dual TB55 (174.6 Wh each) | LiPo 4S, 89.2 Wh |

**Use Case Assignment**

- DJI Phantom 4 Multispectral is primarily deployed for UC-AGR-1 and UC-AGR-2, supporting high-resolution data capture and Normalized Difference Vegetation Index (NDVI) visualization in near real-time.
- DJI Matrice 210 Real-Time Kinematic (RTK) supports heavier payload operations, enabling collaborative use with ground sensors and drone-based grape transport logistics (UC-AGR-3), as well as redundancy in imaging roles (UC-AGR-1 and UC-AGR-2).

Table 4 - Weather Suitability and Environmental Robustness

| Weather Condition | DJI Matrice 210 Real-Time Kinematic (RTK) V1 | DJI Phantom 4 Multispectral |
| --- | --- | --- |
| Wind Resistance | Up to 12 m/s | Not officially specified; lower practical threshold |
| Temperature Range | \-20°C to 45°C | 0°C to 40°C |
| IP Rating | IP43 | None |
| Precipitation Tolerance | Light rain (IP43: protected against water spray up to 60° from vertical) | Not suitable for wet conditions |
| Dust Resistance | Moderate protection | Not rated |

The Matrice 210 Real-Time Kinematic (RTK) is equipped to operate in more demanding environmental conditions, offering higher wind resistance, broader temperature tolerance, and ingress protection (IP43). This makes it better suited for vineyard logistics and monitoring missions that may occur under variable weather conditions. In contrast, the Phantom 4 Multispectral is more appropriate for calm and dry weather, ideal for targeted imaging tasks during favourable conditions. This environmental differentiation further supports the decision to employ a heterogeneous UAV fleet.

In the context of the aerial transport of harvested grapes (UC-AGR-3) requires UAV platforms capable of operating safely and reliably in steep, terraced vineyard terrain while carrying physical payloads. To support early-stage testing and validation of this use case, the DJI Matrice 210 RTK V1 was selected as the initial cargo platform, as previously described.

**Selection Rationale**

The Matrice 210 RTK offers several advantages:

- Payload Capacity: Supports up to 1.57 kg of payload, sufficient for partial grape transport trials and gripper integration testing;
- Precision Landing: Real-Time Kinematic (RTK) module supports centimetre-level accuracy, essential for landing in narrow terrace segments;
- Weather Resistance: Rated IP43 for moderate rain and dust tolerance;
- Platform Stability: Demonstrated robustness under loaded flight conditions in uneven vineyard topography.

While not designed as a dedicated cargo UAV, the Matrice 210 enables real-world testing of UAV-based grape transport under realistic conditions, serving as an adaptable intermediary solution.

**Vineyard Terrain Testing**

- Initial tests were conducted at VMT with the following objectives:
- Lift and Descent at landing location: Performance under near-maximum payload across sloped vineyard plots;
- Obstacle Clearance: Safe navigation near vines and trellis structures, especially on terrace edges;
- Worker Proximity Evaluation: Assessing safe distances for UAV operation when picking up or dropping off grape loads in the presence of workers;
- Landing Accuracy: Evaluated under terrain and space constraints. Due to the narrow inter-row spacing in the vineyard (typically less than 1 meter), landing between vine rows is not feasible. UAVs must instead land on paths that run perpendicular to the vine rows, where slightly wider clearings exist. This requires precise navigation and stable descent control, especially on uneven or sloped ground. The Matrice 210's RTK-based positioning was used to assess its ability to land reliably in these limited zones.

**Simulation-Driven Pre-Validation**

To minimize field-testing risk and optimize mission plans, all transport scenarios were first simulated using the Pegasus simulator integrated with NVIDIA Isaac Sim. The simulations replicated the overall conditions as described in Subsection 3.4.3

- Topographic variability of the vineyard;
- UAV load behavior during motion;
- Approach and landing paths near human agents;
- Altitude modulation across terraces.

The simulation-based preparation ensures that UAV behaviors are pre-evaluated before real deployments, improving safety and efficiency.

# Conclusions/recommendations for full prototype

The Early Prototype (EP) phase of the AI4Work Agriculture Pilot has laid the groundwork for integrating human-centric AI technologies into vineyard environments characterised by steep slopes, mixed varietal distribution, and narrow operational corridors. Focusing on three use cases—grape variety identification (UC-AGR-1), terrain-aware monitoring (UC-AGR-2), and drone-assisted grape transport (UC-AGR-3)—the pilot has validated the feasibility of deploying AI-supported Digital Twins and UAV simulations under realistic field constraints.

This phase combined the adaptation of generic AI4Work components with pilot-specific developments, including UAV-enabled data acquisition pipelines, terrain-linked Digital Twin models, and early integration of spatial monitoring parameters to support context-sensitive mission planning. Given the time-sensitive nature of vineyard operations and the seasonal constraints of field deployment, much of the work completed in this phase is preparatory, designed to inform and de-risk the transition to the full prototype.

Key contributions of the EP include:

- Identification and georeferencing of critical monitoring zones within the VMT;
- Collection of preliminary datasets for grape variety classification, supported by structured metadata;
- Development of a simulation environment for UAV testing using Isaac Sim, PX4, and Pegasus;
- Definition and validation of UAV operating parameters in constrained environments (e.g., altitude clearance, obstacle avoidance, and microclimate tolerances);
- Early formulation of adaptive flight logic and monitoring conditions to support future context-aware reasoning and runtime adaptation.

In parallel, preliminary steps have been taken toward the implementation of the Sliding Work Sharing (SWS) and Long-Term Adaptation (LTA) components. While not yet fully operational within this EP, their requirements are being actively incorporated into UAV simulation scenarios. For instance, environmental constraints and UAV task failures are being used to shape the LTA module’s future planning logic, while shared task allocation workflows between human workers and UAVs are informing the development of SWS logic for collaborative operation.

The EP also revealed challenges to be addressed in subsequent stages:

- Real-world testing of UAV cargo transport remains subject to safety, regulatory, and operational feasibility, and may require extended simulation-based evaluation;
- Context awareness models remain under development, but the current monitoring setup provides structured inputs to facilitate their later integration;
- The adaptation of AI4Work’s mission control and planning architecture to constrained agricultural terrain requires further calibration and human-centred testing.

Looking ahead, the transition to the full prototype will involve:

- Expansion of AI classification models for grape variety identification, using both drone-acquired and synthetic imagery;
- Integration of physiological monitoring data and spatial information into worker-aware task planning;
- Implementation of dynamic UAV scheduling informed by LTA outputs;
- Progressive rollout of Sliding Work Sharing mechanisms, enabling co-adaptive task allocation between human and robotic agents.

In conclusion, the Agriculture Pilot EP provides a robust technical and methodological foundation for AI-assisted vineyard operations. It enables an informed and modular path toward the deployment of AI4Work solutions in real-world agricultural settings—where time, terrain, and safety are critical design factors—and sets the stage for a more adaptive, efficient, and collaborative future in viticultural fieldwork. AI4Work technologies can significantly enhance the grape variety identification (UC-AGR-1), by reducing manual labour, because speeds up the identification, and by helping researchers recognise and protect all the varietal diversity that exist in old vineyards. These technologies can also be valuable for the terrain-aware monitoring (UC-AGR-2), where traditional methods are often labour-intensive and limited by accessibility, by reducing physical strain and possible accidents on steep-slope old vineyards. Moreover, drone-assisted grape transport (UC-AGR-3) can bring benefits, by reducing workers fatigue and manual grape transport, and by making the operation safer and more reliable in challenging terrains.
